{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding Tokenizer and Context Window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visit this link : https://platform.openai.com/tokenizer\n",
    "# to check tokenization in action "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens - indiviual units passed to language model\n",
    "# in early days neural nets we trained on character level\n",
    "# meaning predict the next character in the seq\n",
    "\n",
    "# small vocab size, lesse possibilities at input level\n",
    "# Later, neural nets we started training on word level\n",
    "# predict next word in the sequence\n",
    "# it lead to enormous vocab \n",
    "# so many places for names, places, animals etc...\n",
    "\n",
    "# Rather than training on each word, or each character\n",
    "# a middle ground was achiueved to take the token and work with series of tokens\n",
    "# this had lot of benefits : handling became easier\n",
    "\n",
    "# it was good at handling word stems \n",
    "\n",
    "# you can check GPT tokenization\n",
    "# check how the text is turned into a series of tokens\n",
    "# tokens are highlighted in colours \n",
    "# When you check tokenization , you might also find that the whitespace prefixing the token\n",
    "# is also highlighted as the break between two tokens is equally important\n",
    "\n",
    "# somtimes when you give some rare words, you can find tokens being broken down differently\n",
    "\n",
    "# Refer GPT tokenizer 1 png file to check the break in tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As suggested on Open AI website .. \n",
    "\n",
    "A helpful rule of thumb is that one token generally corresponds to ~4 characters of text for common English text. This translates to roughly Â¾ of a word (so 100 tokens ~= 75 words).\n",
    "\n",
    "Example : \n",
    "The complete works of Shakespeare are ~9,00,000 words or 1.2 million tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is Context Window ? \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1. The main job an LLM is next token prediction.\n",
    "2. Context windows tells you total no.of tokens that an LLM can examine at any point when generating the next token.\n",
    "3. Usually higher the context window for an LM, better its reasoning and logical abilities.\n",
    "4. User prompt and system prompt are inputs to LLMs, which predicts next token.\n",
    "5. It is like a chained reaction to predict next token in case of context window.\n",
    "\n",
    "For example, \n",
    "when you input a prompt to chatgpt, it generates some response. \n",
    "Say after that, you make a follow up question.\n",
    "Now while generating the next token , it will take in all of this as input to generate the next token ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
